conda version 25.3.0 python 3.12.7 loaded.
[09/30/25 17:07:38] WARNING  /N/u/partkaew/BigRed200/.conda/envs/mb2/lib/python3.10/site-packages/tango/step_info.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
                                                                               warnings.py:109
[09/30/25 17:07:38] INFO     Starting new run prime-louse                                                                         site-packages/tango/cli.py:203
[09/30/25 17:07:38] INFO     ✓ Found output for step "raw_text_data" in cache...                                                 site-packages/tango/step.py:748
[09/30/25 17:07:38] INFO     ✓ Found output for step "raw_text_data" in cache...                                                 site-packages/tango/step.py:748
[09/30/25 17:07:44] INFO     ✓ Found output for step "raw_text_data" in cache (needed by "tokenizer")...                         site-packages/tango/step.py:741
[09/30/25 17:07:44] INFO     ● Starting step "tokenizer"...                                                                      site-packages/tango/step.py:761



[09/30/25 17:07:45] INFO     Wrote vocab to./workspace/models/coptic_mx_modern_attention_dropout-0.1_embedding_dropout-0.1_global_attn_every_n_layers-2_hidden_size-128_intermediate_size-192_max_position_embeddings-512_mlp_dropout-0.1_num_attention_heads-4_num_hidden_layers-4MT-MLM-coptic     microbert2/tokenizers.py:29
[09/30/25 17:07:45] INFO     Wrote tokenizer to ./workspace/models/coptic_mx_modern_attention_dropout-0.1_embedding_dropout-0.1_global_attn_every_n_layers-2_hidden_size-128_intermediate_size-192_max_position_embeddings-512_mlp_dropout-0.1_num_attention_heads-4_num_hidden_layers-4MT-MLM-coptic microbert2/data/tokenize.py:187
[09/30/25 17:07:45] INFO     ✓ Finished step "tokenizer"                                                                         site-packages/tango/step.py:774
[09/30/25 17:07:45] INFO     ✓ Found output for step "raw_text_data" in cache...                                                 site-packages/tango/step.py:748
[09/30/25 17:07:51] INFO     ✓ Found output for step "raw_text_data" in cache (needed by "tokenized_text_data")...               site-packages/tango/step.py:741
[09/30/25 17:07:51] INFO     ✓ Found output for step "tokenizer" in cache (needed by "tokenized_text_data")...                   site-packages/tango/step.py:741
[09/30/25 17:07:51] INFO     ● Starting step "tokenized_text_data"...                                                            site-packages/tango/step.py:761
[09/30/25 17:07:51] INFO     Sample inst from mlm_train:

        {'tokens': ['ⲱ', 'ⲡ', 'ⲛⲁⲏⲧ', 'ⲙⲁⲩⲁⲁ', 'ϥ', '·', 'ⲁⲩⲱ', 'ⲡ', 'ⲙⲁⲓ', 'ⲣⲱⲙⲉ', 'ⲛⲓⲙ', '·']}
 microbert2/data/tokenize.py:100
Tokenizing mlm (train):   0%|          | 0/33327 [00:00<?, ?it/s]Tokenizing mlm (train):   1%|1         | 343/33327 [00:00<00:09, 3426.03it/s]Tokenizing mlm (train):   2%|2         | 686/33327 [00:00<00:11, 2863.70it/s]Tokenizing mlm (train):   3%|2         | 979/33327 [00:00<00:12, 2515.42it/s]Tokenizing mlm (train):   4%|3         | 1236/33327 [00:00<00:14, 2226.56it/s]Tokenizing mlm (train):   4%|4         | 1464/33327 [00:00<00:16, 1949.41it/s]Tokenizing mlm (train):   5%|4         | 1665/33327 [00:00<00:17, 1807.07it/s]Tokenizing mlm (train):   6%|5         | 1909/33327 [00:00<00:15, 1972.37it/s]Tokenizing mlm (train):   6%|6         | 2113/33327 [00:01<00:16, 1941.80it/s]Tokenizing mlm (train):   7%|6         | 2312/33327 [00:01<00:16, 1910.17it/s]Tokenizing mlm (train):   8%|7         | 2506/33327 [00:01<00:17, 1809.94it/s]Tokenizing mlm (train):   8%|8         | 2689/33327 [00:01<00:17, 1749.57it/s]Tokenizing mlm (train):   9%|8         | 2884/33327 [00:01<00:16, 1803.26it/s]Tokenizing mlm (train):   9%|9         | 3066/33327 [00:01<00:17, 1724.55it/s]Tokenizing mlm (train):  10%|9         | 3240/33327 [00:01<00:19, 1535.39it/s]Tokenizing mlm (train):  10%|#         | 3398/33327 [00:01<00:19, 1545.26it/s]Tokenizing mlm (train):  11%|#         | 3607/33327 [00:01<00:17, 1691.12it/s]Tokenizing mlm (train):  11%|#1        | 3793/33327 [00:02<00:17, 1735.30it/s]Tokenizing mlm (train):  12%|#1        | 3970/33327 [00:02<00:18, 1620.23it/s]Tokenizing mlm (train):  12%|#2        | 4146/33327 [00:02<00:17, 1655.29it/s]Tokenizing mlm (train):  13%|#2        | 4315/33327 [00:02<00:17, 1623.47it/s]Tokenizing mlm (train):  13%|#3        | 4487/33327 [00:02<00:17, 1649.93it/s]Tokenizing mlm (train):  14%|#4        | 4690/33327 [00:02<00:16, 1755.84it/s]Tokenizing mlm (train):  15%|#4        | 4933/33327 [00:02<00:14, 1945.63it/s]Tokenizing mlm (train):  16%|#5        | 5166/33327 [00:02<00:13, 2053.82it/s]Tokenizing mlm (train):  16%|#6        | 5373/33327 [00:02<00:13, 2014.86it/s]Tokenizing mlm (train):  17%|#6        | 5576/33327 [00:02<00:13, 2017.79it/s]Tokenizing mlm (train):  17%|#7        | 5788/33327 [00:03<00:13, 2047.62it/s]Tokenizing mlm (train):  18%|#7        | 5994/33327 [00:03<00:14, 1937.22it/s]Tokenizing mlm (train):  19%|#8        | 6190/33327 [00:03<00:14, 1837.06it/s]Tokenizing mlm (train):  19%|#9        | 6376/33327 [00:03<00:14, 1814.17it/s]Tokenizing mlm (train):  20%|#9        | 6559/33327 [00:03<00:15, 1765.74it/s]Tokenizing mlm (train):  20%|##        | 6737/33327 [00:03<00:15, 1678.21it/s]Tokenizing mlm (train):  21%|##        | 6906/33327 [00:03<00:16, 1648.14it/s]Tokenizing mlm (train):  21%|##1       | 7072/33327 [00:03<00:16, 1632.05it/s]Tokenizing mlm (train):  22%|##1       | 7236/33327 [00:03<00:16, 1596.83it/s]Tokenizing mlm (train):  22%|##2       | 7396/33327 [00:04<00:16, 1547.51it/s]Tokenizing mlm (train):  23%|##2       | 7554/33327 [00:04<00:16, 1556.52it/s]Tokenizing mlm (train):  23%|##3       | 7710/33327 [00:04<00:16, 1516.67it/s]Tokenizing mlm (train):  24%|##3       | 7862/33327 [00:04<00:17, 1470.13it/s]Tokenizing mlm (train):  24%|##4       | 8010/33327 [00:04<00:17, 1442.61it/s]Tokenizing mlm (train):  24%|##4       | 8155/33327 [00:04<00:17, 1438.01it/s]Tokenizing mlm (train):  25%|##4       | 8300/33327 [00:04<00:17, 1439.97it/s]Tokenizing mlm (train):  25%|##5       | 8445/33327 [00:04<00:17, 1439.28it/s]Tokenizing mlm (train):  26%|##5       | 8599/33327 [00:04<00:16, 1468.66it/s]Tokenizing mlm (train):  26%|##6       | 8773/33327 [00:04<00:15, 1548.09it/s]Tokenizing mlm (train):  27%|##6       | 8945/33327 [00:05<00:15, 1596.60it/s]Tokenizing mlm (train):  27%|##7       | 9105/33327 [00:05<00:15, 1572.43it/s]Tokenizing mlm (train):  28%|##7       | 9263/33327 [00:05<00:15, 1555.81it/s]Tokenizing mlm (train):  28%|##8       | 9431/33327 [00:05<00:15, 1590.15it/s]Tokenizing mlm (train):  29%|##8       | 9591/33327 [00:05<00:15, 1570.09it/s]Tokenizing mlm (train):  29%|##9       | 9749/33327 [00:05<00:15, 1549.68it/s]Tokenizing mlm (train):  30%|##9       | 9905/33327 [00:05<00:15, 1540.46it/s]Tokenizing mlm (train):  30%|###       | 10068/33327 [00:05<00:14, 1565.22it/s]Tokenizing mlm (train):  31%|###       | 10225/33327 [00:05<00:15, 1537.07it/s]Tokenizing mlm (train):  31%|###1      | 10379/33327 [00:06<00:14, 1534.15it/s]Tokenizing mlm (train):  32%|###1      | 10538/33327 [00:06<00:14, 1548.30it/s]Tokenizing mlm (train):  32%|###2      | 10715/33327 [00:06<00:14, 1611.54it/s]Tokenizing mlm (train):  33%|###2      | 10892/33327 [00:06<00:13, 1658.49it/s]Tokenizing mlm (train):  33%|###3      | 11062/33327 [00:06<00:13, 1667.01it/s]Tokenizing mlm (train):  34%|###3      | 11229/33327 [00:06<00:13, 1603.48it/s]Tokenizing mlm (train):  34%|###4      | 11390/33327 [00:06<00:13, 1569.04it/s]Tokenizing mlm (train):  35%|###4      | 11548/33327 [00:06<00:13, 1566.58it/s]Tokenizing mlm (train):  35%|###5      | 11715/33327 [00:06<00:13, 1594.42it/s]Tokenizing mlm (train):  36%|###5      | 11883/33327 [00:06<00:13, 1618.47it/s]Tokenizing mlm (train):  36%|###6      | 12046/33327 [00:07<00:13, 1597.96it/s]Tokenizing mlm (train):  37%|###6      | 12207/33327 [00:07<00:13, 1584.59it/s]Tokenizing mlm (train):  37%|###7      | 12373/33327 [00:07<00:13, 1605.22it/s]Tokenizing mlm (train):  38%|###7      | 12537/33327 [00:07<00:12, 1613.81it/s]Tokenizing mlm (train):  38%|###8      | 12701/33327 [00:07<00:12, 1621.03it/s]Tokenizing mlm (train):  39%|###8      | 12864/33327 [00:07<00:12, 1580.59it/s]Tokenizing mlm (train):  39%|###9      | 13023/33327 [00:07<00:13, 1553.78it/s]Tokenizing mlm (train):  40%|###9      | 13179/33327 [00:07<00:13, 1526.52it/s]Tokenizing mlm (train):  40%|####      | 13345/33327 [00:07<00:12, 1563.96it/s]Tokenizing mlm (train):  41%|####      | 13502/33327 [00:07<00:12, 1552.01it/s]Tokenizing mlm (train):  41%|####      | 13658/33327 [00:08<00:12, 1530.93it/s]Tokenizing mlm (train):  41%|####1     | 13817/33327 [00:08<00:12, 1546.39it/s]Tokenizing mlm (train):  42%|####1     | 13972/33327 [00:08<00:12, 1496.32it/s]Tokenizing mlm (train):  42%|####2     | 14123/33327 [00:08<00:13, 1438.11it/s]Tokenizing mlm (train):  43%|####2     | 14268/33327 [00:08<00:13, 1380.34it/s]Tokenizing mlm (train):  43%|####3     | 14427/33327 [00:08<00:13, 1438.51it/s]Tokenizing mlm (train):  44%|####3     | 14593/33327 [00:08<00:12, 1501.18it/s]Tokenizing mlm (train):  44%|####4     | 14752/33327 [00:08<00:12, 1526.31it/s]Tokenizing mlm (train):  45%|####4     | 14906/33327 [00:08<00:13, 1357.57it/s]Tokenizing mlm (train):  45%|####5     | 15046/33327 [00:09<00:14, 1236.89it/s]Tokenizing mlm (train):  46%|####5     | 15174/33327 [00:09<00:14, 1225.60it/s]Tokenizing mlm (train):  46%|####5     | 15300/33327 [00:09<00:14, 1231.31it/s]Tokenizing mlm (train):  46%|####6     | 15426/33327 [00:09<00:15, 1188.23it/s]Tokenizing mlm (train):  47%|####6     | 15547/33327 [00:09<00:15, 1178.29it/s]Tokenizing mlm (train):  47%|####7     | 15670/33327 [00:09<00:14, 1192.42it/s]Tokenizing mlm (train):  47%|####7     | 15818/33327 [00:09<00:13, 1273.91it/s]Tokenizing mlm (train):  48%|####7     | 15947/33327 [00:09<00:14, 1222.80it/s]Tokenizing mlm (train):  48%|####8     | 16073/33327 [00:09<00:14, 1231.82it/s]Tokenizing mlm (train):  49%|####8     | 16200/33327 [00:10<00:13, 1242.77it/s]Tokenizing mlm (train):  49%|####9     | 16331/33327 [00:10<00:13, 1260.44it/s]Tokenizing mlm (train):  49%|####9     | 16458/33327 [00:10<00:13, 1224.26it/s]Tokenizing mlm (train):  50%|####9     | 16585/33327 [00:10<00:13, 1235.72it/s]Tokenizing mlm (train):  50%|#####     | 16709/33327 [00:10<00:13, 1225.78it/s]Tokenizing mlm (train):  51%|#####     | 16837/33327 [00:10<00:13, 1239.60it/s]Tokenizing mlm (train):  51%|#####     | 16972/33327 [00:10<00:12, 1270.53it/s]Tokenizing mlm (train):  51%|#####1    | 17100/33327 [00:10<00:12, 1268.42it/s]Tokenizing mlm (train):  52%|#####1    | 17228/33327 [00:10<00:12, 1270.94it/s]Tokenizing mlm (train):  52%|#####2    | 17378/33327 [00:10<00:11, 1336.97it/s]Tokenizing mlm (train):  53%|#####2    | 17512/33327 [00:11<00:11, 1328.04it/s]Tokenizing mlm (train):  53%|#####2    | 17645/33327 [00:11<00:11, 1312.88it/s]Tokenizing mlm (train):  53%|#####3    | 17777/33327 [00:11<00:11, 1300.71it/s]Tokenizing mlm (train):  54%|#####3    | 17924/33327 [00:11<00:11, 1348.98it/s]Tokenizing mlm (train):  54%|#####4    | 18089/33327 [00:11<00:10, 1436.06it/s]Tokenizing mlm (train):  55%|#####4    | 18243/33327 [00:11<00:10, 1465.04it/s]Tokenizing mlm (train):  55%|#####5    | 18393/33327 [00:11<00:10, 1474.00it/s]Tokenizing mlm (train):  56%|#####5    | 18541/33327 [00:11<00:10, 1427.28it/s]Tokenizing mlm (train):  56%|#####6    | 18691/33327 [00:11<00:10, 1448.09it/s]Tokenizing mlm (train):  57%|#####6    | 18837/33327 [00:12<00:10, 1399.94it/s]Tokenizing mlm (train):  57%|#####6    | 18978/33327 [00:12<00:10, 1385.61it/s]Tokenizing mlm (train):  57%|#####7    | 19125/33327 [00:12<00:10, 1409.16it/s]Tokenizing mlm (train):  58%|#####7    | 19267/33327 [00:12<00:10, 1352.87it/s]Tokenizing mlm (train):  58%|#####8    | 19403/33327 [00:12<00:10, 1330.34it/s]Tokenizing mlm (train):  59%|#####8    | 19537/33327 [00:12<00:10, 1318.93it/s]Tokenizing mlm (train):  59%|#####9    | 19670/33327 [00:12<00:10, 1319.76it/s]Tokenizing mlm (train):  59%|#####9    | 19803/33327 [00:12<00:10, 1310.01it/s]Tokenizing mlm (train):  60%|#####9    | 19935/33327 [00:12<00:10, 1285.98it/s]Tokenizing mlm (train):  60%|######    | 20064/33327 [00:12<00:10, 1264.31it/s]Tokenizing mlm (train):  61%|######    | 20191/33327 [00:13<00:10, 1233.74it/s]Tokenizing mlm (train):  61%|######    | 20317/33327 [00:13<00:10, 1238.74it/s]Tokenizing mlm (train):  61%|######1   | 20450/33327 [00:13<00:10, 1264.84it/s]Tokenizing mlm (train):  62%|######2   | 20673/33327 [00:13<00:08, 1547.58it/s]Tokenizing mlm (train):  63%|######2   | 20887/33327 [00:13<00:07, 1722.38it/s]Tokenizing mlm (train):  63%|######3   | 21120/33327 [00:13<00:06, 1901.96it/s]Tokenizing mlm (train):  64%|######3   | 21325/33327 [00:13<00:06, 1945.44it/s]Tokenizing mlm (train):  65%|######4   | 21521/33327 [00:13<00:06, 1703.37it/s]Tokenizing mlm (train):  65%|######5   | 21698/33327 [00:13<00:06, 1679.42it/s]Tokenizing mlm (train):  66%|######5   | 21870/33327 [00:14<00:07, 1484.52it/s][09/30/25 17:08:05] WARNING  Stopping at token 493 in sentence with 1290 tokens due to wordpiece limit                            microbert2/data/tokenize.py:47
Tokenizing mlm (train):  66%|######6   | 22025/33327 [00:14<00:08, 1335.69it/s]Tokenizing mlm (train):  67%|######6   | 22165/33327 [00:14<00:08, 1268.38it/s]Tokenizing mlm (train):  67%|######6   | 22296/33327 [00:14<00:09, 1215.41it/s]Tokenizing mlm (train):  67%|######7   | 22421/33327 [00:14<00:09, 1195.08it/s]Tokenizing mlm (train):  68%|######7   | 22543/33327 [00:14<00:09, 1164.74it/s]Tokenizing mlm (train):  68%|######8   | 22710/33327 [00:14<00:08, 1297.45it/s]Tokenizing mlm (train):  69%|######8   | 22843/33327 [00:14<00:08, 1258.63it/s]Tokenizing mlm (train):  69%|######8   | 22985/33327 [00:15<00:07, 1300.73it/s]Tokenizing mlm (train):  69%|######9   | 23117/33327 [00:15<00:07, 1293.13it/s]Tokenizing mlm (train):  70%|######9   | 23258/33327 [00:15<00:07, 1324.50it/s]Tokenizing mlm (train):  70%|#######   | 23426/33327 [00:15<00:06, 1425.69it/s]Tokenizing mlm (train):  71%|#######   | 23618/33327 [00:15<00:06, 1568.17it/s]Tokenizing mlm (train):  71%|#######1  | 23776/33327 [00:15<00:06, 1537.70it/s]Tokenizing mlm (train):  72%|#######1  | 23951/33327 [00:15<00:05, 1598.82it/s]Tokenizing mlm (train):  72%|#######2  | 24151/33327 [00:15<00:05, 1714.83it/s]Tokenizing mlm (train):  73%|#######3  | 24350/33327 [00:15<00:05, 1793.81it/s]Tokenizing mlm (train):  74%|#######3  | 24588/33327 [00:15<00:04, 1965.21it/s]Tokenizing mlm (train):  74%|#######4  | 24822/33327 [00:16<00:04, 2074.90it/s]Tokenizing mlm (train):  75%|#######5  | 25056/33327 [00:16<00:03, 2149.62it/s]Tokenizing mlm (train):  76%|#######5  | 25272/33327 [00:16<00:03, 2124.90it/s]Tokenizing mlm (train):  76%|#######6  | 25485/33327 [00:16<00:03, 2096.91it/s]Tokenizing mlm (train):  77%|#######7  | 25695/33327 [00:16<00:03, 2097.05it/s]Tokenizing mlm (train):  78%|#######7  | 25905/33327 [00:16<00:03, 2082.91it/s]Tokenizing mlm (train):  78%|#######8  | 26114/33327 [00:16<00:03, 2071.62it/s]Tokenizing mlm (train):  79%|#######8  | 26324/33327 [00:16<00:03, 2079.23it/s]Tokenizing mlm (train):  80%|#######9  | 26544/33327 [00:16<00:03, 2110.89it/s]Tokenizing mlm (train):  80%|########  | 26764/33327 [00:16<00:03, 2136.11it/s]Tokenizing mlm (train):  81%|########  | 26978/33327 [00:17<00:03, 1931.91it/s]Tokenizing mlm (train):  82%|########1 | 27204/33327 [00:17<00:03, 2021.41it/s]Tokenizing mlm (train):  82%|########2 | 27410/33327 [00:17<00:02, 2015.82it/s][09/30/25 17:08:09] WARNING  Stopping at token 505 in sentence with 1003 tokens due to wordpiece limit                            microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 500 in sentence with 751 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 495 in sentence with 694 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 489 in sentence with 529 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 502 in sentence with 685 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 506 in sentence with 688 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 504 in sentence with 846 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 497 in sentence with 1088 tokens due to wordpiece limit                            microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 493 in sentence with 740 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 501 in sentence with 646 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 494 in sentence with 614 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 495 in sentence with 767 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 509 in sentence with 526 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 499 in sentence with 871 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 499 in sentence with 624 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 501 in sentence with 722 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 491 in sentence with 605 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
Tokenizing mlm (train):  83%|########2 | 27614/33327 [00:17<00:04, 1285.34it/s][09/30/25 17:08:09] WARNING  Stopping at token 502 in sentence with 841 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
[09/30/25 17:08:09] WARNING  Stopping at token 500 in sentence with 743 tokens due to wordpiece limit                             microbert2/data/tokenize.py:47
Tokenizing mlm (train):  83%|########3 | 27778/33327 [00:17<00:04, 1357.95it/s]Tokenizing mlm (train):  84%|########3 | 27942/33327 [00:17<00:03, 1359.24it/s]Tokenizing mlm (train):  84%|########4 | 28101/33327 [00:17<00:03, 1413.01it/s]Tokenizing mlm (train):  85%|########4 | 28288/33327 [00:18<00:03, 1526.10it/s]Tokenizing mlm (train):  85%|########5 | 28462/33327 [00:18<00:03, 1581.05it/s]Tokenizing mlm (train):  86%|########6 | 28696/33327 [00:18<00:02, 1788.35it/s]Tokenizing mlm (train):  87%|########6 | 28901/33327 [00:18<00:02, 1861.52it/s]Tokenizing mlm (train):  87%|########7 | 29094/33327 [00:18<00:02, 1778.56it/s]Tokenizing mlm (train):  88%|########7 | 29302/33327 [00:18<00:02, 1859.59it/s]Tokenizing mlm (train):  88%|########8 | 29493/33327 [00:18<00:02, 1771.52it/s]Tokenizing mlm (train):  89%|########9 | 29698/33327 [00:18<00:01, 1847.88it/s]Tokenizing mlm (train):  90%|########9 | 29886/33327 [00:18<00:01, 1841.31it/s]Tokenizing mlm (train):  90%|######### | 30075/33327 [00:18<00:01, 1854.49it/s]Tokenizing mlm (train):  91%|######### | 30263/33327 [00:19<00:01, 1860.84it/s]Tokenizing mlm (train):  91%|#########1| 30467/33327 [00:19<00:01, 1912.78it/s]Tokenizing mlm (train):  92%|#########2| 30692/33327 [00:19<00:01, 2011.93it/s]Tokenizing mlm (train):  93%|#########2| 30896/33327 [00:19<00:01, 2011.09it/s]Tokenizing mlm (train):  93%|#########3| 31102/33327 [00:19<00:01, 2023.52it/s]Tokenizing mlm (train):  94%|#########3| 31310/33327 [00:19<00:00, 2037.93it/s]Tokenizing mlm (train):  95%|#########4| 31515/33327 [00:19<00:00, 1973.33it/s]Tokenizing mlm (train):  95%|#########5| 31722/33327 [00:19<00:00, 1999.49it/s]Tokenizing mlm (train):  96%|#########5| 31923/33327 [00:19<00:00, 1731.84it/s]Tokenizing mlm (train):  96%|#########6| 32103/33327 [00:20<00:00, 1697.17it/s]Tokenizing mlm (train):  97%|#########6| 32306/33327 [00:20<00:00, 1784.94it/s]Tokenizing mlm (train):  98%|#########7| 32537/33327 [00:20<00:00, 1927.08it/s]Tokenizing mlm (train):  98%|#########8| 32734/33327 [00:20<00:00, 1819.17it/s]Tokenizing mlm (train):  99%|#########8| 32920/33327 [00:20<00:00, 1797.60it/s]Tokenizing mlm (train):  99%|#########9| 33103/33327 [00:20<00:00, 1730.26it/s]Tokenizing mlm (train): 100%|#########9| 33278/33327 [00:20<00:00, 1638.35it/s]Tokenizing mlm (train): 100%|##########| 33327/33327 [00:20<00:00, 1607.56it/s]
[09/30/25 17:08:12] INFO     Split train: 33327 sentences, 977966 tokens, 1058613 wordpieces, 20 discarded                       microbert2/data/tokenize.py:131
[09/30/25 17:08:12] INFO     Sample inst from mlm_dev:

        {'tokens': ['ⲁⲛⲟⲕ', 'ⲡ', 'ⲣⲙⲣⲁϣ', 'ⲙⲁⲣⲉ', 'ϥ', 'ϣⲱⲡⲉ', 'ⲛ', 'ⲣⲉϥⲙⲓϣⲉ', 'ⲙⲛ', 'ⲛⲓⲙ', '·']}
 microbert2/data/tokenize.py:100
Tokenizing mlm (dev):   0%|          | 0/1284 [00:00<?, ?it/s]Tokenizing mlm (dev):  27%|##7       | 351/1284 [00:00<00:00, 3503.36it/s]Tokenizing mlm (dev):  55%|#####4    | 702/1284 [00:00<00:00, 3418.59it/s]Tokenizing mlm (dev):  81%|########1 | 1044/1284 [00:00<00:00, 2941.92it/s]Tokenizing mlm (dev): 100%|##########| 1284/1284 [00:00<00:00, 2726.03it/s]
[09/30/25 17:08:12] INFO     Split dev: 1284 sentences, 21389 tokens, 24264 wordpieces, 0 discarded                              microbert2/data/tokenize.py:131
[09/30/25 17:08:12] INFO     Sample inst from mlm_test:

        {'tokens': ['ⲉϩⲣⲁⲓ', 'ⲉ', 'ⲡ', 'ⲉⲧ', 'ϩⲓⲧⲟⲩⲱⲱ', 'ϥ', '·']}
            microbert2/data/tokenize.py:100
Tokenizing mlm (test):   0%|          | 0/1284 [00:00<?, ?it/s]Tokenizing mlm (test):  27%|##7       | 351/1284 [00:00<00:00, 3506.69it/s]Tokenizing mlm (test):  55%|#####4    | 702/1284 [00:00<00:00, 3452.97it/s]Tokenizing mlm (test):  82%|########1 | 1048/1284 [00:00<00:00, 2965.56it/s]Tokenizing mlm (test): 100%|##########| 1284/1284 [00:00<00:00, 2747.67it/s]
[09/30/25 17:08:13] INFO     Split test: 1284 sentences, 21389 tokens, 24264 wordpieces, 0 discarded                             microbert2/data/tokenize.py:131
[09/30/25 17:08:13] INFO     Sample inst from mt_train:

        {   'tgt_attention_mask': '...',
    'tgt_input_ids': '...',
    'tokens': [   'ⲁⲩⲱ', 'ⲛⲧⲉⲣⲉ', 'ϥ', 'ⲛⲁⲩ', 'ⲉⲣⲟ', 'ϥ', 'ⲛϭⲓ', 'ⲡ', 'ϩⲗⲗⲟ', 'ⲁ', 'ϥ', 'ⲁⲥⲡⲁⲍⲉ', 'ⲙⲙⲟ', 'ϥ', 'ϩⲛ',
                  'ⲟⲩ', 'ⲡⲉⲓ', 'ⲉ', 'ⲥ', 'ⲟⲩⲁⲁⲃ', '·']}
 microbert2/data/tokenize.py:100
Tokenizing mt (train):   0%|          | 0/15470 [00:00<?, ?it/s]Tokenizing mt (train):   2%|1         | 273/15470 [00:00<00:05, 2728.51it/s]Tokenizing mt (train):   4%|4         | 654/15470 [00:00<00:04, 3360.45it/s]Tokenizing mt (train):   6%|6         | 991/15470 [00:00<00:04, 2990.60it/s]Tokenizing mt (train):   8%|8         | 1295/15470 [00:00<00:05, 2581.05it/s]Tokenizing mt (train):  10%|#         | 1562/15470 [00:00<00:05, 2391.53it/s]Tokenizing mt (train):  12%|#1        | 1823/15470 [00:00<00:05, 2450.38it/s]Tokenizing mt (train):  13%|#3        | 2073/15470 [00:00<00:06, 2117.63it/s]Tokenizing mt (train):  15%|#4        | 2294/15470 [00:00<00:06, 1952.07it/s]Tokenizing mt (train):  16%|#6        | 2496/15470 [00:01<00:06, 1950.87it/s]Tokenizing mt (train):  17%|#7        | 2696/15470 [00:01<00:06, 1903.56it/s]Tokenizing mt (train):  19%|#8        | 2890/15470 [00:01<00:06, 1879.16it/s]Tokenizing mt (train):  20%|#9        | 3091/15470 [00:01<00:06, 1911.87it/s]Tokenizing mt (train):  21%|##1       | 3297/15470 [00:01<00:06, 1952.35it/s]Tokenizing mt (train):  23%|##2       | 3532/15470 [00:01<00:05, 2064.55it/s]Tokenizing mt (train):  24%|##4       | 3741/15470 [00:01<00:05, 1972.85it/s]Tokenizing mt (train):  25%|##5       | 3941/15470 [00:01<00:06, 1830.89it/s]Tokenizing mt (train):  27%|##6       | 4127/15470 [00:01<00:06, 1748.73it/s]Tokenizing mt (train):  28%|##7       | 4304/15470 [00:02<00:06, 1753.04it/s]Tokenizing mt (train):  29%|##9       | 4540/15470 [00:02<00:05, 1919.09it/s]Tokenizing mt (train):  31%|###       | 4735/15470 [00:02<00:05, 1894.36it/s]Tokenizing mt (train):  32%|###1      | 4927/15470 [00:02<00:05, 1885.78it/s]Tokenizing mt (train):  33%|###3      | 5117/15470 [00:02<00:05, 1748.92it/s]Tokenizing mt (train):  34%|###4      | 5295/15470 [00:02<00:05, 1723.59it/s]Tokenizing mt (train):  36%|###5      | 5501/15470 [00:02<00:05, 1817.27it/s]Tokenizing mt (train):  37%|###6      | 5685/15470 [00:02<00:05, 1650.92it/s]Tokenizing mt (train):  38%|###7      | 5854/15470 [00:03<00:06, 1491.16it/s]Tokenizing mt (train):  39%|###9      | 6035/15470 [00:03<00:06, 1571.77it/s]Tokenizing mt (train):  40%|####      | 6245/15470 [00:03<00:05, 1711.16it/s]Tokenizing mt (train):  42%|####1     | 6421/15470 [00:03<00:05, 1690.45it/s]Tokenizing mt (train):  43%|####2     | 6594/15470 [00:03<00:05, 1624.31it/s]Tokenizing mt (train):  44%|####3     | 6768/15470 [00:03<00:05, 1655.75it/s]Tokenizing mt (train):  45%|####4     | 6936/15470 [00:03<00:05, 1611.99it/s]Tokenizing mt (train):  46%|####6     | 7118/15470 [00:03<00:05, 1669.78it/s]Tokenizing mt (train):  47%|####7     | 7316/15470 [00:03<00:04, 1758.28it/s]Tokenizing mt (train):  49%|####8     | 7538/15470 [00:03<00:04, 1892.03it/s]Tokenizing mt (train):  50%|#####     | 7772/15470 [00:04<00:03, 2022.24it/s]Tokenizing mt (train):  52%|#####1    | 7995/15470 [00:04<00:03, 2081.88it/s]Tokenizing mt (train):  53%|#####3    | 8214/15470 [00:04<00:03, 2113.43it/s]Tokenizing mt (train):  54%|#####4    | 8427/15470 [00:04<00:03, 2064.88it/s]Tokenizing mt (train):  56%|#####5    | 8635/15470 [00:04<00:03, 1962.11it/s]Tokenizing mt (train):  57%|#####7    | 8852/15470 [00:04<00:03, 2018.44it/s]Tokenizing mt (train):  59%|#####8    | 9056/15470 [00:04<00:03, 2022.72it/s]Tokenizing mt (train):  60%|#####9    | 9260/15470 [00:04<00:03, 1884.24it/s]Tokenizing mt (train):  61%|######1   | 9466/15470 [00:04<00:03, 1932.39it/s]Tokenizing mt (train):  62%|######2   | 9662/15470 [00:05<00:03, 1815.00it/s]Tokenizing mt (train):  64%|######3   | 9866/15470 [00:05<00:02, 1875.05it/s]Tokenizing mt (train):  65%|######5   | 10056/15470 [00:05<00:02, 1861.15it/s]Tokenizing mt (train):  66%|######6   | 10252/15470 [00:05<00:02, 1882.84it/s]Tokenizing mt (train):  67%|######7   | 10442/15470 [00:05<00:02, 1871.45it/s]Tokenizing mt (train):  69%|######8   | 10655/15470 [00:05<00:02, 1942.55it/s]Tokenizing mt (train):  70%|#######   | 10881/15470 [00:05<00:02, 2035.27it/s]Tokenizing mt (train):  72%|#######1  | 11086/15470 [00:05<00:02, 2011.15it/s]Tokenizing mt (train):  73%|#######2  | 11288/15470 [00:05<00:02, 2010.36it/s]Tokenizing mt (train):  74%|#######4  | 11499/15470 [00:05<00:01, 2038.49it/s]Tokenizing mt (train):  76%|#######5  | 11704/15470 [00:06<00:01, 1972.37it/s]Tokenizing mt (train):  77%|#######6  | 11904/15470 [00:06<00:01, 1979.81it/s]Tokenizing mt (train):  78%|#######8  | 12103/15470 [00:06<00:01, 1695.48it/s]Tokenizing mt (train):  79%|#######9  | 12280/15470 [00:06<00:02, 1571.48it/s]Tokenizing mt (train):  80%|########  | 12444/15470 [00:06<00:01, 1581.76it/s]Tokenizing mt (train):  82%|########1 | 12618/15470 [00:06<00:01, 1620.28it/s]Tokenizing mt (train):  83%|########2 | 12791/15470 [00:06<00:01, 1649.97it/s]Tokenizing mt (train):  84%|########3 | 12959/15470 [00:06<00:01, 1640.01it/s]Tokenizing mt (train):  85%|########4 | 13125/15470 [00:06<00:01, 1598.60it/s]Tokenizing mt (train):  86%|########5 | 13287/15470 [00:07<00:01, 1600.30it/s]Tokenizing mt (train):  87%|########6 | 13455/15470 [00:07<00:01, 1622.30it/s]Tokenizing mt (train):  88%|########8 | 13619/15470 [00:07<00:01, 1617.19it/s]Tokenizing mt (train):  89%|########9 | 13782/15470 [00:07<00:01, 1588.26it/s]Tokenizing mt (train):  90%|######### | 13942/15470 [00:07<00:00, 1584.99it/s]Tokenizing mt (train):  91%|#########1| 14106/15470 [00:07<00:00, 1597.49it/s]Tokenizing mt (train):  92%|#########2| 14268/15470 [00:07<00:00, 1602.85it/s]Tokenizing mt (train):  93%|#########3| 14444/15470 [00:07<00:00, 1644.18it/s]Tokenizing mt (train):  95%|#########4| 14649/15470 [00:07<00:00, 1764.22it/s]Tokenizing mt (train):  96%|#########5| 14826/15470 [00:07<00:00, 1722.27it/s]Tokenizing mt (train):  97%|#########6| 14999/15470 [00:08<00:00, 1710.86it/s]Tokenizing mt (train):  98%|#########8| 15171/15470 [00:08<00:00, 1704.86it/s]Tokenizing mt (train):  99%|#########9| 15342/15470 [00:08<00:00, 1645.11it/s]Tokenizing mt (train): 100%|##########| 15470/15470 [00:08<00:00, 1843.79it/s]
[09/30/25 17:08:21] INFO     Split train: 15470 sentences, 393416 tokens, 428220 wordpieces, 0 discarded                         microbert2/data/tokenize.py:131
[09/30/25 17:08:21] INFO     Sample inst from mt_dev:

        {'tgt_attention_mask': '...', 'tgt_input_ids': '...', 'tokens': ['ϫⲉ', 'ⲁ', 'ⲩ', 'ⲕⲱϩ', 'ⲉⲣⲟ', 'ϥ', '·']}
                                                                                                                               microbert2/data/tokenize.py:100
Tokenizing mt (dev):   0%|          | 0/1331 [00:00<?, ?it/s]Tokenizing mt (dev):  25%|##4       | 329/1331 [00:00<00:00, 3271.32it/s]Tokenizing mt (dev):  49%|####9     | 658/1331 [00:00<00:00, 3282.23it/s]Tokenizing mt (dev):  74%|#######4  | 987/1331 [00:00<00:00, 1187.33it/s]Tokenizing mt (dev):  91%|######### | 1205/1331 [00:00<00:00, 1376.14it/s]Tokenizing mt (dev): 100%|##########| 1331/1331 [00:00<00:00, 1541.87it/s]
[09/30/25 17:08:22] INFO     Split dev: 1331 sentences, 22239 tokens, 25218 wordpieces, 0 discarded                              microbert2/data/tokenize.py:131
[09/30/25 17:08:22] ERROR    Uncaught exception
Traceback (most recent call last):
  File "/N/u/partkaew/BigRed200/.conda/envs/mb2/lib/python3.10/site-packages/tango/step.py", line 483, in _run_with_work_dir
    result = self.run(**kwargs)
  File "/geode2/home/u070/partkaew/BigRed200/microbert2/microbert2/data/tokenize.py", line 146, in run
    task_dataset = {
  File "/geode2/home/u070/partkaew/BigRed200/microbert2/microbert2/data/tokenize.py", line 147, in <dictcomp>
    k: self._process_split(v, k, task.slug, tokenizer, max_length, discard_truncated=True)
  File "/geode2/home/u070/partkaew/BigRed200/microbert2/microbert2/data/tokenize.py", line 99, in _process_split
    sample = pprint.pformat(random.choice(split), indent=4, width=120, compact=True)
  File "/N/u/partkaew/BigRed200/.conda/envs/mb2/lib/python3.10/random.py", line 378, in choice
    return seq[self._randbelow(len(seq))]
IndexError: list index out of range
[09/30/25 17:08:22] ERROR    ✗ Step "tokenized_text_data" failed                                                                 site-packages/tango/step.py:782
[09/30/25 17:08:22] ERROR    ✗ Run prime-louse finished with errors                                                               site-packages/tango/cli.py:208
                                                                                                                                                                
 ┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
 ┃ Step Name           ┃ Status      ┃ Results                                                                                                                 ┃
 ┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
 │ model_inputs        │ - not run   │ N/A                                                                                                                     │
 │ raw_text_data       │ ✓ succeeded │ /geode2/home/u070/partkaew/BigRed200/microbert2/workspace/cache/ReadWhitespaceTokenizedText-RorW7HNdn4QQxufoHAaazqPKtq… │
 │ tokenized_text_data │ ✗ failed    │ N/A                                                                                                                     │
 │ tokenizer           │ ✓ succeeded │ /geode2/home/u070/partkaew/BigRed200/microbert2/workspace/cache/TrainTokenizer-62CJEqNyW1LeAFew5GhaUsXQBqqwX3iu         │
 │ trained_model       │ - not run   │ N/A                                                                                                                     │
 └─────────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
                                                              ✗ 1 failed, ✓ 2 succeeded, 2 not run                                                              
                                                                                                                                                                
